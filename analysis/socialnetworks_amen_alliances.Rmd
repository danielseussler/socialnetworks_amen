---
title: The Additive and Multiplicative Effects Network Model
subtitle: Seminar on Statistical Modeling of Social Networks
author: Daniel A. Seussler Becerra
date: "June 2021"
output:
  bookdown::pdf_document2: 
    fig_caption: yes
classoption: twoside
fontsize: 11pt
toc: false
geometry: margin=1in 
bibliography: 
  - references.bib
header-includes:
  - \usepackage{float}
  - \usepackage{url}
  - \usepackage{setspace}\onehalfspacing
  - \usepackage{natbib}\bibliographystyle{apa}
  - \interfootnotelinepenalty=10000 
  - \usepackage[ruled,vlined]{algorithm2e} 
keywords: Social Networks, Dyadic Data, Bayesian estimation, Latent Factors, International Relations.
abstract: "This seminar paper introduces the Additive and Multiplicative Effects (AME) network model as summarized in @Hoff2021. First, a general introduction to network and dyadic data is given along with the statistical challenges related to this type of data analysis. Then, the AME network model is introduced and motivated as a Bayesian regression approach to network analysis. Second, an application case to the interstate alliance network in 2000 is presented. A direct comparison of restricted approaches using posterior predictive fits displays the lack of fit of conventional regression models and validates the AME network model. Further approaches are explored to improve model fit, yet issues with the sampling procedure in the Bayesian setting persist and raise doubts about the validity of statistical inference in this application case. The paper concludes with a discussion of the limitations and opportunities of the chosen approach. *Keywords: Network Data, Bayesian Estimation, Latent Factors, International Relations.*"
thanks: "Replication files are available on Github (http://github.com/danielseussler). **Current version**: `r format(Sys.time(),'%B %d, %Y')`; **Corresponding author**: daniel.seussler@campus.lmu.de"
---

\newpage
# Introduction 
Networks are ubiquitous in political science, economics, and sociology, but a variety of applications can also be found in natural sciences. In its most basic form, we consider a set of actors with relationships between each other. A relationship between two nodes is called a dyad^[Node and dyad are commonly named actor and tie in the Social Sciences, I use the names interchangeably. In Mathematical Graph Theory, I would refer to vertices $V$ with edges $E$ linking two vertices at a time. The set $G = (V, E)$ is called a graph $G$.] or a dyadic observation, hence the name dyadic data analysis. This requires specific statistical approaches, the center of research in statistical network analysis. Three aspects of networks warrant further research. First, clustering or community detection could be of interest in explorative data analysis. Second, network composition, uncovering determinants of relationship formation could yield information about the structure of interest. Third, the dynamic evolution of networks can be informative.

One prominent approach is presented here, the Additive and Multiplicative Effects (AME) Network Model. This model has been introduced in @Hoff2021 and is based on previous work in @Hoff2007 and @Hoff2005. It lies in the intersection of mentioned research approaches; in that, it can describe patterns in networks while accounting for the effects of external variables and influences of network effects. To accomplish this, an extended error component is embedded in a generalized linear regression framework, designed to capture the additional statistical dependencies of observations inherent in network or dyadic data. 

In panel data analysis, we are concerned that the repeated measurement of observed units could introduce bias into our estimates, as it violates the conditional indendence assumption, which requires adjustments to the models employed. Similarly, the composition of network or dyadic data, specifically the possibility, that an actors' relationship with another actor, can depend on the latter ones' relationships in the network, does require adapted models. To capture these effects, I classify three types of network effects, also called statistical dependencies in networks, based on the number of nodes involved. Consider the relationships of actor $i$ to actors $j$, $k$, and $l$, stated by the set $\{y_{ij}, y_{ik}, y_{il}\}$. Then this set could exhibit higher similarity *within* itself than with other dyadic observations, due to the common actor $i$. I. e., we could see heterogeneity in the propensity to send (or receive) network ties, depending on if the actor is the sender of the receiver of a given network tie. These are first-order network effects that are observed on a nodal or actor level. Involving two nodes, i.e., dyadic relationships, we could see patterns of reciprocity. This is characterized by the statistical correlation between observation $y_{ij}$ and $y_{ji}$. Third, there is reason to assume dependence involving triadic relationships within a given network. These could be transitivity, balance, and clustering effects, and are best described by proverbs such as *the enemy of my enemy is my friend* and *my friends' friend is also my friend*.

The AME network model is able to capture these types of network effects. It has thus found its way into the applied network analysis literature. @violenceNetworks apply this model to study civil conflict in Nigeria and the implications of the emergence of Boko Haram on the conflict structure. In @ameSyria it is taken to study the Syrian civil conflict. The AME model framework has also been expanded to include longitudinal networks or replicated data. @Minhas_Hoff_Ward_2016 use a time series approach of the AME network model presented in @hoff2015longitudinal to study international relations, in this case, the type of interaction chosen by each state. The time dependency is modeled using a vector autoregression (VAR) approach. @dynamicAMEN analyse UN Voting Behaviour in a similar fashion with a longitudinal extension of the AME framework, relying on Gaussian processes.

The rest of this paper is structured in two sections. In the following theory section, I introduce common network statistics for the above-illustrated network effects and the AME network model as specified in @Hoff2021. Then, in an application section, I use data of the interstate alliance network in 2000 and apply the beforehand introduced AME network model. A comparison of alternative restricted specifications stresses the necessity of network approaches in the analysis of network data. The seminar paper concludes with a discussion on the limitations and further approaches.


# Theory
Let $Y \in \mathbb{R}^{n \times n}$ for network actors labeled $1, \dots, n$, where $y_{ij}$ describes the directed relationship between actor $i$ and $j$. The former is taken to be the sender, the latter the receiver of the network tie. Note that, it is possible to construct a network matrix from any dyadic data set $\{y_{ij}: \: i,j = 1, \dots, n\}$ by taking the relationship $y_{ij}$ from actor $i$ to $j$ as the $ij$-th entry of $Y$ and vice versa. Hence, both terms can be used interchangeably. Because I assume the actors to be acting (or embedded) within a network environment, there is *a priori* no reason to assume (conditional) independence of the observations $y_{ij}$.^[The observation $y_{ij}$ refers to the $ij$-th observation in the matrix $Y$. Capital letters will be used to emphasise the matrix structure and boldface for vector variables.] For a given matrix $Y$ we define the following network statistics. Note that, we rule out self-ties within a network, i.e., for any given matrix representing a network we assume the diagonal to be undefined. A network matrix with this properties is commonly referred to as a sociomatrix. Undefined observations should therefore be dropped in the following calculations.

*First-order effects* characterize the actor specific heterogeneity in sending (or receiving) ties within a network. In a homogenous sender setting, we would expect the row means $\bar{Y}_{i \cdot}$ to be similar (or column means). Heterogeneity in sending or receiving ties is therefore summarised by the standard deviation of the respective row and column means. Let $\{\mu_i: \: 1, \dots, n\}$ be the the set of rowmeans, $\{\eta_i: \: 1, \dots, n\}$ of columnmeans. The first-order network statistics are given as
$$sd.rowmean(Y) = \frac{1}{n} \sum^{n}_{i = 1} \left( \mu_i - \bar{\mu_i} \right)^2 \text{, and   } sd.colmean(Y) = \frac{1}{n} \sum^{n}_{i = 1} \left( \eta_i - \bar{\eta_i} \right)^2.$$
Presence of actor-specific heterogeneity can also be tested using an ordinary ANOVA F-test for comparison of group means. 

*Second-order effects*, i.e., reciprocity, describe the statistical dependency of the directed relationship between two actors in the network. For the relationship between actor $i$ and $j$, the correlation with the relationship of actor $j$ to $i$, if $i \neq j$, captures this effect. For a network it is given as 
$$dyadic.dependency(Y) = corr\left( vec(Y), vec\left( Y^\top \right) \right).$$
Here, $vec(A)$ is a $m n \times 1$ column vector, which is obtained by stacking the columns of $A \in \mathbb{R}^{n \times m}$ on top of another. Naturally, for an undirected network dyadic dependency is 1. 

*Third-order effects* are patterns among three nodes and corresponding statistics shall capture triadic relationships as described above. Let $E = Y - \bar{Y}$. Then 
$$cycle.dependency(Y) = \frac{1}{n (n-1) (n-2)} \sum_{i \neq j \neq k \neq i} E_{ij} E_{jk} E_{ki},$$
$$trans.dependency(Y) = \frac{1}{n (n-1) (n-2)} \sum_{i \neq j \neq k \neq i} E_{ij} E_{jk} E_{ik}.$$
Optionally the statistics can be standardized with $sd(vec(Y))^3$. Naturally, for an undirected network cycle dependency equals transitive dependency. 

Lastly, actors $i$ and $j$ are *stochastically equivalent*, if both exhibit a similar probability to send (or receive) network ties with other groups of actors. This contrasts with *homophily* between actors, which describes a higher likelihood of a network tie based on similar characteristics. 

In the process of model selection, the network statistics above can be used to assess model fit. Based on draws from the posterior distribution (as later a Bayesian approach to model estimation is followed2), sociomatrices are simulated and respective goodness of fit statistics computed. This yields a distribution of network statistics which is in turn compared to the observed statistic. Ideally, the respective histograms are centered around the observed statistic, indicating the capability of the model to generate networks that do exhibit this degree of network effects.

The Additive and Multiplicative Effects network model is based on two types of random effects, additive and multiplicative, which are added to an error component in a generalized linear regression framework. The inclusion guarantees that the error structure considers the types of network dependencies inherent in dyadic observations as specified above. I introduce both separately and start with the additive component.


## Additive Effects: The Social Relations Regression Model
To accommodate actor-specific heterogeneity beyond fixed covariates, sender and receiver additive random effects $(a_i, b_j)$ are included in an extended error component. Let $\mu$ be a mean or a linear predictor $\boldsymbol{\beta}^\top \mathbf{x}_{ij}$.^[The $p$-dimensional covariate vector $\mathbf{x}_{ij}$ may include nodal or dyadic specific values.] This yields the following model, which is also referred to as the Social Relations Regression (SRRM) model:
$$
\begin{aligned}
      y_{ij} &= \mu + e_{ij} \\
      e_{ij} &= a_{i} + b_{j} + \epsilon_{ij} \\
      \{ (a_{1}, b_{1}), \ldots, (a_{n}, b_{n}) \} &\sim \mathcal{N}_2(\mathbf{0},\Sigma_{ab}), \\ 
      \{ (\epsilon_{ij}, \epsilon_{ji}) : \; i \neq j\} &\sim \mathcal{N}_2(\mathbf{0},\Sigma_{\epsilon}),\\
      \Sigma_{ab} = \begin{pmatrix} 
      \sigma_{a}^{2} & \sigma_{ab} \\ 
      \sigma_{ab} & \sigma_{b}^2   \end{pmatrix}, 
      \quad \: &\Sigma_{\epsilon} = \sigma_{\epsilon}^{2} 
      \begin{pmatrix} 1 & \rho \\ 
      \rho & 1  \end{pmatrix}.
\end{aligned}
$$
for $\{ 1 \leq i , j \leq n, i \neq j \}$.  Parameter interpretation is straightforward. Nodal random effects $a_i$, $b_j$ are independent normal gaussian variables with zero mean and covariance matrix $\Sigma_{ab}$ for $i, j = 1, \dots, n$. The covariance matrix $\Sigma_{\epsilon}$ of the error term $\epsilon$ is composed of the error-variance $\sigma_{\epsilon}^{2}$ and a reciprocity parameter $\rho$. Moreover
$$
\begin{aligned}
      Cov[Y_{ik}, Y_{il}] & = \sigma^2_a, \\
      Cov[Y_{ik}, Y_{lk}] & = \sigma^2_b, \\
      Cov[Y_{ik}, Y_{kl}] & = \sigma_{ab}, \\
      Cov[Y_{ij}, Y_{ji}] & = 2 \sigma_{ab} + \rho \sigma^2,
\end{aligned}
$$
are within-row variance, within-column variance, and row-column variance without and with reciprocity.

## Multiplicative Effects: The Latent Factor Model
Nodal random effects can capture first- and second-order network effects but cannot account for triadic relationships within the data. This is illustrated in the application case below and can also be seen as follows. For an estimated model the extended error component $\hat{e}_{ij}=\hat{a}_i + \hat{b}_j + \hat{\epsilon}_{ij}$ is 0 in expectation, and because it is approximate normal gaussian its third moments equal 0. Therefore, an SRRM model will *not* capture third-order network dependencies by design. This issue is alleviated by the inclusion of latent factors as multiplicative effect for network nodes. The idea stems from the Latent Space Model, where network actors are associated in a lower-dimensional space through latent variables. In a Latent Space approach with dimension $r$, define an additional component: 
$$\alpha(\mathbf{u}_i, \mathbf{u}_j) = -\vert \mathbf{u}_i-\mathbf{u}_j\vert, \qquad \mathbf{u}_i \in \mathbb{R}^r, \qquad i = 1, \dots, n.$$ 
The variable $\mathbf{u}_i$ is a vector of unobserved characteristics for actor $i$. Yet, the Latent Space Approach comes with one caveat. If $\mathbf{u}_l, \mathbf{u}_k$ are close in the latent space, then $\vert \mathbf{u}_l-\mathbf{u}_k \vert$ is small. But then for every other actor with latent variable $\mathbf{u}_j$ holds$\vert \mathbf{u}_l-\mathbf{u}_j \vert \approx\vert \mathbf{u}_k- \mathbf{u}_j \vert$ which implies a similar stochastic pattern to the third actor. Thus, this approach cannot distinguish between homophily and stochastic equivalence. As most networks in practice exhibit both types of effect of varying degree, a change which does account for both is required.

A multiplicative component is therefore specified as a latent factor, as introduced in @Hoff2005. Let $r \in \mathbb{N}$ be the dimension of the latent factor, each for the sender and receiver, i.e., $\mathbf{u}_{i}, \mathbf{v}_{j} \in \mathbb{R}^{r}$ for $i,j = 1, \dots, n$. It can be interpreted as unobserved characteristics of the actor as sender or receiver. The term
$$\alpha(\mathbf{u}_{i}, \mathbf{v}_{j}) = \mathbf{u}_{i}^{T} \mathbf{v}_{j}$$
is the multiplicative component. In a symmetric sociomatrix, we introduce $L$ as an $r \times r$ diagonal matrix, i.e.,
$$\alpha(\mathbf{u}_{i}, \mathbf{u}_{j}) = \mathbf{u}_{i}^{T} L \mathbf{u}_{j}$$
to properly generalize symmetric matrices. Higher values of $\alpha$ indicate higher likelihood of a tie and depend on the diagonal values of $L$ aswell as $u_{ik}$ and $v_{kj}$ for $k \in \{1, \dots, r\}$. Determining the dimension of the latent space is not a trivial task and is to be considered alongside other modeling decisions. However, selecting lower rank dimensions facilitates visualization of the estimated effects, which can be informative by itself. To minimise overfitting and permit interpretation as random effects, let 
$$\{ (\mathbf{u}_{1}, \mathbf{v}_{1}), \ldots, (\mathbf{u}_{n}, \mathbf{v}_{n}) \} \sim \mathcal{N}_{2r} (\mathbf{0},\Psi).$$ 
This multiplicative effect is introduced in @Hoff2007 as the latent eigenmodel, and contrasted to the Latent Space model (above) and a latent class model. In a Latent Class model, every actor is associated with a latent class, and actors of the same class exhibit similar patterns of tie formation with other classes. Both models can capture certain types of network effects, however, the Latent Factor model can generalize both of them, as proven in @Hoff2007.


## The AME Model: Additive plus Multiplicative Effects
Merging both types of random effects yields the Additive and Multiplicative network effects model, which can account for the network effects described above. The model is specified as
$$
\begin{aligned}
      y_{ij} &= \beta^{T} \mathbf{x}_{ij} + e_{ij} \\
      e_{ij} &= a_{i} + b_{j}  + \alpha(\mathbf{u}_{i}, \mathbf{v}_{j}) + \epsilon_{ij}     
\end{aligned}
$$
for $\{ 1 \leq i , j \leq n, i \neq j \}$ and $a_i$, $b_j$, $\alpha(\mathbf{u}_{i}, \mathbf{v}_{j})$ with the respective covariance structures as defined above. Using matrices, the model can be rewritten for an appropiate $M$ as
$$Y = M(X, \boldsymbol{\beta}) + \mathbf{a} \mathbf{1}^\top + \mathbf{1} \mathbf{b}^\top + U V^\top$$
and 
$$Y = M(X, \boldsymbol{\beta}) + \mathbf{a} \mathbf{1}^\top + \mathbf{1} \mathbf{a}^\top + ULU^\top$$
in the symmetric case. Again, I take the diagonal to be not defined in matrix notation. In fact, as the algorithm presented below does take this into account, it is viable to handle missing values off the diagonal. This is a welcome feature for observational studies, where data may be missing at random. Until now the directed tie $y_{ij}$ is taken to be a continuous outcome variable in $\mathbb{R}$. Of course, this is not necessary, and the model can be extended to include other outcome variables as well, see the part on further extensions below. This does require the introduction of a link function, analogous to a generalized linear regression approach, as well as modifications to the estimation strategy. Generally, the AME network model is estimated using a Bayesian approach with Markov chain Monte Carlo (MCMC) simulation, which is described in the following part. I restrict this again to continuous outcome variables. 

An empirical comparison to other prominent approaches, such as the Exponential Random Graph Model, can be found in @minhas_hoff_ward_2019. 


## Parameter Estimation
The MCMC estimation approach is achieved through the implementation of a Gibbs sampling algorithm, summarized for the AME *without* multiplicative effects in Algorithm 1. This is in turn extended to include the multiplicative effects in Algorithm 2. All details and derivations for the full conditional distributions necessary to implement this sampler are outlined in @Hoff2021. Here, I provide only the (semi-) conjugate prior- and full conditionals for each step. First, let 
$$
\begin{aligned}  
  \beta &\sim \mathcal{N}_p(\boldsymbol{\beta_0}, Q^{-1}_0),\\
  \frac{1}{\sigma^2} &\sim \text{Gamma} \left(\frac{\nu_0}{2}, \frac{\nu_0 \sigma^2}{2} \right),\\
  \Sigma^{-1} &\sim \text{Wishart} \left( \frac{\Sigma^{-1}_0}{\eta_0}, \eta_0 \right)
\end{aligned} 
$$
be our prior distributions for appropiate $\beta_0, Q_0, \nu_0, \Sigma_0, \eta_0$. Note that I consider $\Sigma_{\epsilon}$ through $\sigma^2$ and $\rho$, as well as $\Sigma := \Sigma_{ab}$. 
```{=latex}
\begin{algorithm}[H]
Initalize unkown variables\;
1. Simulate $\{ \boldsymbol{\beta}, \mathbf{a, b} \}$ given $Y, \Sigma, \sigma^2, \rho$\;
2. Simulate $\sigma^2$ given $Y, \boldsymbol{\beta}, \mathbf{a, b}, \rho$\;
3. Simulate $\rho$ given $Y, \boldsymbol{\beta}, \mathbf{a, b}, \sigma^2$\; 
4. Simulate $\Sigma$ given $\mathbf{a, b}$\; 
5. Simulate missing values of $Y$ given $\boldsymbol{\beta}, \mathbf{a, b}, \rho, \sigma^2$ and observed values of $Y$.
 \caption{Gibbs Sampling for the SRRM, P. D. Hoff (2021).}
\end{algorithm}
```
To approximate a posterior distribution by the way of Gibbs Sampling, knowledge of every full conditional, i.e., the probability distribution of one parameter conditioned on *every* other parameter, has to be available. Thus, below I provide every full conditional required for this estimation strategy. Starting from *Step 2*, let
$$
SS_1 = \sum_{i<j} (e_{ij} e_{ji}) \begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix} 
\begin{pmatrix} e_{ij} \\ e_{ji} \end{pmatrix}, \qquad
SS_2 = \sum^{n}_{i=1} \frac{1}{1+ \rho} e^2_{ii},
$$
and $e_{ij}$ the $ij$-th value of $E = Y - (M(X, \boldsymbol{\beta}) + \mathbf{a} \mathbf{1}^\top + \mathbf{1} \mathbf{b}^\top)$. Then the full conditional for the error variance is:
$$
\frac{1}{\sigma^2} \: \vert \: Y, \boldsymbol{\beta}, \mathbf{a, b}, \rho \sim \text{Gamma} \left( \frac{1}{2} \left(\nu_0 + n^2 \right), \frac{1}{2} \left( \nu_0 \sigma^2_0 + SS_1 + SS_2 \right) \right)
$$
For *Step 3*, note that $\rho \in [-1, 1]$, thus an empirical CDF sampler on $[-1, 1]$ is sufficient to approximate the posterior for $\rho$. This circumvents the issue of specifying a prior distribution. The covariance matrix of the additive random effects in *Step 4* is simulated using
$$
\Sigma^{-1} \: \vert \: [\mathbf{a}, \mathbf{b}] \sim \text{Wishart}\left( (\eta_0 \Sigma_0 + C^\top C)^{-1} , \eta_0 + n     \right)
$$
where $C= [\mathbf{a}, \mathbf{b}] \in \mathbb{R}^{n \times 2}$ is the columnwise concatenation of $\mathbf{a}, \mathbf{b}$. *Step 5* requires a definition by cases, as there can be three different constellations of missing data. First, values on the diagonal, a missing pair $(y_{ij}, y_{ji})$, and half a missing pair. Hence,
$$
\begin{aligned}
y_{ii} \: \vert \: \boldsymbol{\beta}, \mathbf{a, b}, \rho, \sigma^2 &\sim \mathcal{N} \left( m_{ij} + a_i + b_j, \: \sigma^2 (1+\rho) \right),\\
(y_{ij} \: y_{ji})^\top   \: \vert \: \boldsymbol{\beta}, \mathbf{a, b}, \rho, \sigma^2 &\sim \mathcal{N}_2\left(
\begin{pmatrix} m_{ij} + a_i + b_j \\ m_{ji} + a_j + b_i  \end{pmatrix}, \:
\sigma^2 \begin{pmatrix} 1 & \rho \\ \rho & 1  \end{pmatrix}
\right),\\
y_{ji} \: \vert \: y_{ij}, \boldsymbol{\beta}, \mathbf{a, b}, \rho, \sigma^2 &\sim \mathcal{N} \left( \rho ( y_{ij}-m_{ij}-a_i-b_j) + m_{ji}+a_j+b_i, \: \sigma^2 \left(1-\rho^2\right) \right).
\end{aligned}
$$
And lastly, the simulation of $\{\boldsymbol{\beta}, \mathbf{a, b}\}$, *Step 1*. This step requires an additional transformation of the data and is done for $\boldsymbol{\beta}$ and $\{\mathbf{a, b}\}$ seperately. Consider the transformation of $Y$:
$$
\begin{aligned}
\tilde{c} &= \frac{1}{2\sigma} \left(   (1 + \rho)^{-1/2} + (1 - \rho)^{-1/2} \right) \\
\tilde{d} &= \frac{1}{2\sigma} \left( (1 + \rho)^{-1/2} - (1 - \rho)^{-1/2}  \right) \\
\tilde{Y} &= \tilde{c}Y + \tilde{d}Y^\top 
\end{aligned}
$$
which yields the following model. It does does not exhibit dyadic correlation in the error term $Z$, which facilitates the derivation of the full conditionals. Noted in matrix type equations:  
$$
\begin{aligned}
\tilde{Y} &= M (\tilde{X}, \boldsymbol{\beta}) + \mathbf{\tilde{a}}\mathbf{1}^\top + \mathbf{1}\mathbf{\tilde{b}}^\top + Z\\
\tilde{x}_{ij} &= \tilde{c} x_{ij} + \tilde{d} x_{ji} \\
Z &\sim \mathcal{N}_{n \times n}(\mathbf{0}, I)\\
(\tilde{a}_1, \tilde{b}_1), \dots, (\tilde{a}_n, \tilde{b}_n) &\sim \mathcal{N}_2 (\mathbf{0}, \tilde{\Sigma})\\
\tilde{\Sigma} &= \Sigma^{-1/2}_e \Sigma \Sigma^{-1/2}_e
\end{aligned}
$$
It is then possible to simulate $\{\beta, \tilde{a}, \tilde{b} \}$ and obtain $(a_i, b_i)^\top = \Sigma^{1/2}_e (\tilde{a}_i, \tilde{b}_i)^\top$ by retransforming into the original space. Dropping the $\sim$ briefly to facilitate notation, let $\mathbf{y}$ be the vectorization of $Y = M(X, \boldsymbol{\beta})\mathbf{a}\mathbf{1}^\top + \mathbf{1}\mathbf{v}^\top + Z$, i.e.,
$$\mathbf{y} = \mathbf{m} + W C + \mathbf{z}$$
with $C= [\mathbf{a}, \mathbf{b}]$ as before. Define:
$$G = \left( \Sigma^{-1} + nI \right)^{-1}, \text{ and  } H = \left( \Sigma^{-1} + n\mathbf{1}\mathbf{1}^\top \right)^{-1} \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} G$$
The full conditional to simulate $\mathbf{a}, \mathbf{b}$ is then
$$[\mathbf{a}, \mathbf{b}] \: \vert \: Y, M, \Sigma \sim \mathcal{N} \left(  \left( W^\top W + \Sigma^{-1} \otimes I \right)^{-1} W^\top (\mathbf{y}-\mathbf{m}), \: G \otimes I - H \otimes 11^\top \right).$$
The sign $\otimes$ denotes the Kronecker Product. Retransformation yiels $[\mathbf{a}, \mathbf{b}]$. To simulate $\boldsymbol{\beta}$ let
$$
\begin{aligned}
Q_1 &= X^\top X\\
Q_2 &= n^4 h \bar{x} \bar{x}^\top\\
Q_3 &= - n^2 \left(g_{11} \bar{X}^{\top}_r \bar{X}_r + g_{12}(\bar{X}^{\top}_r \bar{X}_c + \bar{X}^{\top}_c \bar{X}_r) + g_{22}\bar{X}^{\top}_c \bar{X}_c \right) \\
\mathbf{l_1} &= X^\top y\\
\mathbf{l_2} &= n^4 h \bar{x} \bar{y}, \quad h= \mathbf{1}^\top H \mathbf{1} \\
\mathbf{l_3} &= - n^2 \left(g_{11} \bar{X}^{\top}_r \bar{y}_r + g_{12}(\bar{X}^{\top}_r \bar{y}_c + \bar{X}^{\top}_c \bar{y}_r) + g_{22}\bar{X}^{\top}_c \bar{y}_c \right)
\end{aligned}
$$
and $Q = Q_1 + Q_2 + Q_3$, $\mathbf{l = l_1 + l_2 + l_3}$. Then the full conditional density is given as
$$\boldsymbol{\beta} \vert y, \Sigma \sim \mathcal{N}_p \left( (Q_0 + Q)^{-1}(Q_0\mathbf{\beta_0 + l}), \: (Q_0 + Q)^{-1}\right).$$
Adding multiplicative effects to the SRRM model is accomplished as follows.
```{=latex}
\begin{algorithm}[H]
Initalize unkown variables\;
1. Update $\{ \boldsymbol{\beta}, \mathbf{a, b}, \sigma^2, \rho, \Sigma \}$ and the missing values of $Y$ using Algorithm 1, but with $Y$ replaced by $Y$ - $UV^T$\;
2. Simulate $\Psi^{-1} \sim$ Wishart $((\Psi_0 \kappa_0 + [UV]^\top [UV])^{-1}, \kappa_0 + n)$, where $[UV]$ is the $n \times 2r$ matrix equal to the column-wise concatenation of $U$ and $V$\;
3. For each $k = 1, \dots, r$, simulate the $r$-th columns of $U$ and $V$ from their full conditional distributions. 
 \caption{Gibbs Sampling for the AME, P. D. Hoff (2021).}
\end{algorithm} 
```
Only *Step 3* requires an additional full conditional, the derivation is illustrated in the case of $u_1$ and the rest is constructed analogous. Define the residual as 
$$R = Y - \left(M(X,\boldsymbol{\beta}) + \sum_{k=2}^{r} u_k v_k^\top + \mathbf{a}\mathbf{1}^\top + \mathbf{1}\mathbf{b}^\top \right) = u_1 v_1^\top + E.$$
Again, to cirumvent issues with the dyadic correlation, $R$ is decorrelated using the transformation introduced above.
$$
\begin{aligned}
\tilde{R} &= \tilde{c}R + \tilde{d}R^\top = \tilde{c} u_1 v_1^\top + \tilde{d}v_1u_1^\top + Z\\
\mathbf{\tilde{r}} &= (\tilde{c}(v_1 \otimes I) + \tilde{d} I \otimes v_1))u_1 + \mathbf{z}
\end{aligned}
$$
The second equation is the vectorization of $R$. Using the conditional mean and variance of $\mathbf{u}_1$ given $\mathbf{v}_1$, this yields the conditional distribution
```{=latex}
\vspace{-1cm}
\begin{multline*}
u_1 \vert \tilde{R}, v_1 \sim \mathcal{N} \bigg( \Sigma^{-1}_{u\vert v} + \left(\tilde{c}^2 \tilde{d}^2) ||v_1||^2 I + 2 \tilde{c} \tilde{d} v_1 v_1^\top \right)^{-1}, \\
( \Sigma^{-1}_{u\vert v} + (\tilde{c}^2 \tilde{d}^2) ||v_1||^2 I + 2 \tilde{c} \tilde{d} v_1 v_1^\top)^{-1} \left( \Sigma^{-1}_{u\vert v} \mu_{u \vert v} + (\tilde{c}R + \tilde{d}R^\top) v_1 \right) \bigg).
\end{multline*}
```
Which concludes the section.

## Extensions
Note that by now a normal distribution of the outcome variable is assumed. However, the presented framework can be extended in several different ways. For one thing, additional types of outcome variables can be specified, such as binary, count, ordinal, and censored data. Ordinal data includes ranked categorical outcomes, for example, *reject, neutral, agree*.  This requires changes to the sampling algorithms described above. For the specific changes to the algorithms and full conditionals, as required in the binary case for the following application case, see @Hoff2021. Furthermore, estimation of missing data points could be an objective. In the Bayesian estimation approach, imputation of missing data is easily available through posterior simulation, here in Algorithm 1 Step 5. A third possibility is an extension to include longitudinal or replicated networks, where a (possibly changing) set of actors is observed at different time points. One approach to include replicated data is taking it into account as lagged variables. A different approach is presented in @dynamicAMEN, where the AME network model is embedded in a Gaussian process.

The estimation approach above as well as these extensions have been implemented in the R-package `amen` from @ramen.


# Application Case: Interstate Defence Alliances
To illustrate the purpose of statistical network approaches let us turn our attention to the field of international relations. More specifically, I apply the beforehand introduced AME network model to the interstate defence alliances network in the year 2000. Alliance formation in an interstate framework provides an interesting application case, both because of the availability of data and the assumed extent of network effects inherent in the strategic decisions of state actors. Neglecting the latter effects by presuming conditional independence of observations, as in the classical regression framework, could significantly bias parameter estimates (@statIssue).

The data is an excerpt from the data used in @ComplexDependencies and @TowardNetwork and contains the yearly interstate defence network as a time series from 1981 to 2000.^[The data set `alliances` can be conveniently accessed as network data in the R-Package `xergm.common`.] It furthermore includes information on military capabilities, political regime types, geographic borders, and interstate conflict. From here on out, I refer to this data set as the alliances data set. 

This case study is structured as follows. First, restricted to a cross-sectional study of the year 2000, I examine the alliances' structure with respect to the network statistics and preliminary models. Second, to motivate the statistical network approach, I estimate four different specifications and compare these on their goodness of fit statistics. I conclude with a discussion of the limitations and further approaches of the AME network model. The computational analysis was implemented with the R-Software (@rsoftware) and R-package `amen` from @ramen.

```{r setup, echo=FALSE, message=FALSE}
# These options are tuned for manuscript/presentation.
# They basically run R in the background except for spitting out figures/tables

knitr::opts_chunk$set(
	echo = FALSE,
	error = FALSE,
	message = FALSE,
	warning = FALSE
)

library(amen)
library(xergm.common)
library(network)
library(statnet)
library(kableExtra)
library(tidyverse)
data("alliances")
set.seed(42)

```

```{r output_help, include=FALSE}
# Modified summary function of amen package to output a table easier to handle
table_ame <- function(object, ...){ 
  fit <- object
  tmp <- cbind(
    apply(fit$BETA, 2, mean), apply(fit$BETA, 2, sd),
    
    2 * (1 - pnorm(abs(apply(fit$BETA, 2, mean) / apply(fit$BETA, 2, sd))))
  )
  colnames(tmp) <- c("pmean", "psd", "p - value")
  out <- round(tmp, 3)
  
  
  tmp <- cbind(apply(fit$VC, 2, mean), apply(fit$VC, 2, sd))
  tmp <- cbind(round(tmp, 3), array(" ", dim = c(nrow(tmp), 1)))

  out <- rbind(out, tmp)
  return(out)
} 
```

```{r alliancesfigure, fig.cap = "The interstate defence alliance network in the year 2000. Countries with no interstate alliance recorded are ommitted from the plot, as well as some country names for better readability.", fig.align = "center", out.height="95%"}
knitr::include_graphics("figures/alliancesplot.pdf")
```


## Network Statistics
The alliances data set includes 158 countries^[This data set covers the period of 1981 - 2000, it includes all countries which were selected at any time of given series. I exclude the Yemen Arab Republic, Yemen People's Republic, German Democratic Republic, German Federal Republic, Yugoslavia, and Czechoslovakia as former countries, which yields a total of 158 countries.] and a total of 767 observed interstate alliances. In the corresponding sociomatrix, a value of 1 indicates the presence of an alliance between two countries, 0 its absence. Note that we have a symmetric matrix, as an alliance between two states is taken to be not directed. 55 countries have no alliance recorded in the data set, the United States and Canada declare the most, with 43 and 39 alliances respectively. The median number of alliances is 9 and the network density is 0.061.

A visualization of the interstate alliance network is provided in Figure \@ref(fig:alliancesfigure). Strong clustering based on geographic proximity is immediately apparent. Located in the upper right of the figure we find the Middle East, below France in the center are the European states. On the left, we find the former USSR countries, in the lower part Latin America and the Caribbean states. Perhaps more surprisingly there are two distinct clusters of African states, beyond the North African states forming a cluster with the Middle Eastern countries.

Observed statistics of network dependence will be used later on to assess model fit and are computed as described above. First-order statistics, the standard deviation of the row- and column means is 0.0596. An ANOVA F-test for equality of row- or columnmeans yields an F-statistic of 10.967. The Null Hypothesis of equality of row- or columnmeans is thus rejected. As our sociomatrix is symmetric, second-order dependency (reciprocity) is 1. Third-order statistics are 0.3881, both for the cycle and transitive dependency, because of symmetry. I. e., the network does exhibit network-specific statistical dependencies and a modeling approach should reflect this.


## Model Comparison: Interstate Defence Alliances
I follow the specification of @dynamicAMEN and @geometryofsecurity to define nodal and dyadic covariates which are both sensitive to the approach and relevant to the theoretic discussion of interstate alliance networks.^[@dynamicAMEN present a dynamic extension of the AME network model. In @geometryofsecurity the author proceeds with a *stochastic actor-oriented* approach for the analysis of longitudinal network data. The covariates of interest reflect the research field, but are not directly comparable.] 

*1. Nodal Covariates.* As covariate of interest-specific to each actor in the network, I include Gross Domestic Product (GDP) in per capita terms and on a log scale for each country using data from @wdigdppc. Missing data points are imputed on an individual basis.^[Adding the *Composite Index of National Capability* (CINC) as a measure for military capability, appended in the alliances data set, did not improve model fit during preliminary modeling. It is therefore omitted from the analysis.]

*2. Dyadic Covariates.* As dyadic effects I include covariates on geographic factors, political and military similarity, past conflicts, economic dependence, and cultural similarity. The covariates are defined as follows.

1. $GeoDistance_{ij}$ is the logarithmic geographic distance between two countries measured by the distance of the respective capitals. If two countries share a border, the log distance is set 0. 
2. $CulturalSim_{ij}$ is a dichotomous variable which takes the value 1, if the most spoken language of two countries is the same, 0 else.^[I use data from the replication files of @geometryofsecurity and choose the year 1985, since the year 2000 did not seem to be accurate. @culturalsimdata is provided as the original data source.] 
3. $EconomicDep_{ij}$ is a measure for the economic dependence of two states, calculated by the share of Imports and Exports to the respective GDP: 
  $$EconomicDep_{ij} = min\left(\frac{Trade_{ij}}{GDP_i}, \frac{Trade_{ij}}{GDP_j} \right) \cdot 100\%$$
  To construct this index, I take data from the Correlates of War Project on trade flows (@tradedata) and @wdigdp. Missing data points for GDP are imputed on individual basis, negative values for trade are set to 0. 
4. $SharedAllies_{ij}$ is the number of shared allies between country $i$ and $j$. 
5. $ConflictInd_{ij}$ is an indicator variable that takes the value 1 if a militarized interstate dispute was recorded in the preceding 10 years between country $i$ and $j$, 0 else.
6. $PoliticalSim_{ij}$ is a measure for the political dissimilarity of country $i$ and $j$ and is constructed using data from the Polity IV Index: 
  $$PoliticalSim_{ij} = \lvert POLITY_i - POLITY_j \rvert$$
  The Polity IV index classifies countries on a 21 point scale from -10 to 10, where -10 to -6 corresponds to autocracies, -5 to 5 to anocracies, and 6 to 10 to democracies. Note that a high value in *PoliticalSim* corresponds to a high *dissimilarity*.
7. $CapabilityRat_{ij}$ is a capability ratio and defined as the log of the relative *Composite Index of National Capability* of both countries, with the stronger state (indicated by the subscript *s*) as the numerator: 
  $$CapabilityRat_{ij} = \log\left(\frac{CINC_s}{CINC_w}\right).$$
At the provided precision, three countries are indistinguishable from 0. I impute these data points with 0.5 times of the second to lowest value.

Note that each dyadic fixed effect is symmetric. To validate the network approach taken in this application case, I estimate four models with different restrictions. One with additive and multiplicative effects (AME), one with only additive effects (AE), one with only multiplicative effects (ME), and one without any types of random effects (NO). The latter one yields a classical (bayesian) probit regression, which does not take into account any type of network effect and assumes conditional indepedence of observations. This serves as a benchmark for our validation. Based on preliminary modeling, I choose rank two for the multiplicative effects.^[A two-dimensional latent factor space facilitates the visualization of the multiplicative effects. Moreover, choosing a higher dimension of R = 5 did not improve model fit considerably (not shown here, see code repository for further comparisons).] The results presented in this section are based on 100,000 iterations and a burn-in of 10,000. Output density is set at 100. Long sampling and stark thinning are due to symmetric matrices of the multiplicative part, which result in considerably slow mixing of the MCMC chains [see @hoff2015dyadic, p. 44] and high autocorrelation of the estimates.

As elaborated in the theory section, model assessment is performed using the goodness of fit statistics of posterior predictive draws. Figure \@ref(fig:gofstats) illustrates this comparison for our models. The red line indicates the observed goodness of fit statistic for the sociomatrix. On the left, we have a comparison of first-order network statistics, on the right, third-order. As our matrix is symmetric, second-order dependence, i.e. reciprocity, is constant, obsolete and thus omitted.

The first row compares the ordinary (bayesian) probit regression (NO) with the additive effects (AE) model. Predictably, the former (pink) does barely represent the network dependencies present in the data. Adding additive effects does improve this representation (blue). An overestimated first-order effect does probably account for the high third-order network effect. Still, both models are not able to capture the extent of statistical dependency in the network. In the second row, the additive effects (AE) model is compared to the multiplicative effects model (ME, R = 2, green), which does improve model fit considerably. First-order effects are captured very well, representation of third-order effects has improved. Comparing multiplicative effects with the full AME Model (grey, third row), both R = 2, show the best fit to the goodness of fit statistics. Furthermore, the lack of fit of the probit regression stresses the issues of ordinary regression approaches applied to network data, as the model is not capable of reproducing the network structure present in the data set. In this case, bias in the estimates is to be expected.

```{r models}
fitZNII0H <- readRDS(file = "models/fitZNII0H.rds") # AME R=2
fitCPLTUK <- readRDS(file = "models/fitCPLTUK.rds") # Additive Effects
fitX7XDFO <- readRDS(file = "models/fitX7XDFO.rds") # Multiplicative Effects R = 2
fitDASD8R <- readRDS(file = "models/fitDASD8R.rds") # No Effects
```

```{r gofstats, fig.cap = "Pairwise comparison (row-wise) of posterior predictive distributions of the goodness of fit statistics for the ordinary regression (pink), only additive effects (blue), only multiplicative effects (green), and both additive and multiplicative effects (grey).", fig.align = "center", out.height="90%"}
knitr::include_graphics("figures/gofstats.pdf")
```

```{r modelcomparison1, results='asis'}
modelAMEvsNO <- cbind(table_ame(fitZNII0H), 
                      table_ame(fitDASD8R))
rownames(modelAMEvsNO)[1] <- "Intercept"

modelAMEmsg <- paste("Effective Sample Size of the AME Network Model (R=2): ", toString(round(coda::effectiveSize(fitZNII0H$BETA))), ".", sep = "")
modelNOmsg <- paste("Effective Sample Size of the Probit Regression: ", toString(round(coda::effectiveSize(fitDASD8R$BETA))), ".", sep = "") 

modelAMEvsNO[10:11, 4:5] <- " "

kbl(modelAMEvsNO, booktabs = T, caption = "Model Comparison") %>%
  kable_styling(full_width = T) %>%
  column_spec(1, width = "4cm") %>%
  pack_rows("AME Components", 10, 11, latex_gap_space = "1em") %>%
  add_header_above(c(" " = 1 , "AME Network Model (R=2)" = 3, "Probit Regression" = 3)) %>%
  footnote(number = c(modelAMEmsg, modelNOmsg))
```

```{r modelcomparison2, results='asis'}
modelAEvsME <- cbind(table_ame(fitCPLTUK), 
                     table_ame(fitX7XDFO))
rownames(modelAEvsME)[1] <- "Intercept"

modelAMmsg <- paste("Effective Sample Size of the Additive Effects Model: ", toString(round(coda::effectiveSize(fitCPLTUK$BETA))), ".", sep = "") 
modelMEmsg <- paste("Effective Sample Size of the Multiplicative Effects Model: ", toString(round(coda::effectiveSize(fitX7XDFO$BETA))), ".", sep = "") 

modelAEvsME[10:11, 4:5] <- " "

kbl(modelAEvsME, booktabs = T, caption = "Model Comparison") %>%
  kable_styling(full_width = T) %>%
  column_spec(1, width = "4cm") %>%
  pack_rows("AME Components", 10, 11, latex_gap_space = "1em") %>%
  add_header_above(c(" " = 1 , "Additive Effects" = 3, "Multiplicative Effects (R=2)" = 3)) %>%
  footnote(number = c(modelAMmsg, modelMEmsg))
```

Table \@ref(tab:modelcomparison1) and \@ref(tab:modelcomparison2) provide parameter estimates^[All values in the following tables are rounded to the nearest thousandth.] for the four specifications discussed above. Estimates are provided as posterior means, along with the posterior standard deviation and a $p$ - value.^[The $p$ - value is calculated under the assumption of an approximately normal posterior distribution, i.e., $z = \frac{pmean}{psd} \sim \mathcal{N}(0,1)$. Yet, the $p$ - value should be interpreted with caution, as the density plots do not always suggest approximate normality.] Different specifications lead to vastly different estimates, not only in magnitude but also in changes of the sign. For example, the probit regression does underestimate the effect of economic dependence and political similarity in comparison to the AME network model.

We restrict our attention to the AME network model with rank two as, based on the network dependencies in Figure \@ref(fig:gofstats), it exhibits the best fit in the above comparison. In the AME framework, we can interpret the fixed effects similar to the generalized linear regression framework, with the addition that we condition the effect estimate on the additive and multiplicative random effects. All estimates are directionally consistent with our expectations. A low intercept reflects the low density (0.061) of the interstate alliance network. A higher GDP (log p. c.) is associated *ceteris paribus* and conditional on the random effects with a higher propensity of interstate alliance formation. Geographic distance has a negative effect. Cultural similarity as reflected by joint language, a higher economic dependence, number of shared allies, and previous conflict are all associated with the presence of alliances between two states. In political similarity, a higher dissimilarity is associated with a negative effect on alliance formation, whereas differences in military capability show no effect.

In the AME network model framework, statistical network dependencies will be encoded in the additive and multiplicative, i.e., the random-effects, component of the model. We expect the multiplicative component to capture third-order network effects and cluster types of states which share similar characteristics with respect to their tie formation in the network to other groups, while additive effects should capture the actor-specific propensity of tie formation. Figure \@ref(fig:latentfactors) visualizes the latent space obtained with the AME (R=2) model. The multiplicative effect (matrix notation) in a symmetric model is $U L U^\top$, where $U \in \mathbb{R}^{158 \times 2}$ and $L \in \mathbb{R}^{2 \times 2}$. Both $U$ and $L$ are provided as posterior means in the package output, and to visualize the latent factors a decomposition of this matrix is computed: $\mathbf{u}_{i} \sqrt{L}$ for $i = 1, \dots, 158$, where $\mathbf{u}_{i}= U_{i \cdot}.$

Apart from the scatter in the center of the plot, the multiplicative component captures some interesting geographical factors, which were already apparent in Figure \@ref(fig:alliancesfigure), and are not captured within the $GeoDistance_{ij}$ fixed effect. This includes the Americas on the left and at least two distinct clusters of African states. Russia and some of the former USSR states are clustered in the lower left, the right side has the Middle East and North African countries. 

Additive effects are plotted in Figure \@ref(fig:additiveeffects). Among the five countries with the highest posterior mean additive effects are Canada, the United States, and France. Switzerland, Sweden, and Austria list among the five countries with the lowest values. This is as expected, as additive effects are added to capture first-order effects, i.e., the propensity to form a network tie.

```{r latentfactors, fig.cap = "Visualization of the latent factor space in the AME network model with rank 2. Values are posterior mean estimates for the multiplicative random effects. Proximity in the latent space indicates similarity with respect to tie formation.", fig.align = "center", out.width="100%"}
knitr::include_graphics("figures/latentfactors.pdf")
```

```{r additiveeffects, fig.cap = "Additive Random Effects in the AME network model with rank 2. Values are posterior mean estimates for the additive random effects. AE effects capture first-order network effects, i.e., actor-specific heterogeneity in the propensity to form a network tie.", out.width="100%"}
knitr::include_graphics("figures/additiveeffectsplot.pdf")
```

```{r traceplotAMEr2, fig.cap ="Trace plots for the AME rank 2 model. Trending and multimodality is visible for several parameter trace plots of the MCMC estimation.", fig.align='center', out.height="90%"}
knitr::include_graphics("figures/traceplotAmeR2.pdf")
```

Trace plots for the nine fixed effects and the variance of the additive effect (VA) for this estimation are shown in Figure \@ref(fig:traceplotAMEr2). Both multimodality and persistent trending are apparent, even *after* strong thinning of 1 to 100. Specifically for the variance of the additive effect and the intercept, which suggests issues with the mixing of the MCMC chains. This is also indicated by the low effective sample size (Table \@ref(tab:modelcomparison1)) for the specifications including multiplicative effects.

To adapt the model, I estimated the following changed specifications of the AME rank 2 model. First, I dropped the Intercept, which yields worse effective sample sizes. Predictably, the variance of the additive effect increased, along with a change of sign in the fixed effects estimates of *GDP (log p.c.), ConflictInd*, and *CapabilityRat*. Overall it did not seem to improve the estimation.^[This and the following model can be found in the online code repository and are not shown here.]

Next, I dropped the two covariates with the lowest effective sample size, *SharedAllies* and *ConflictInd*. Except for the intercept, which increased by a third, the estimated variance of the additive effect and the fixed effect estimates remained virtually the same. The issues with the trace plots and the low effective sample size remained. 

Then, I removed all non-significant fixed covariates, leaving only the *Intercept, GeoDistance, EconomicDep, SharedAllies*, and *PoliticalSim*. Again, these changes did not seem to substantially improve the model, the results are shown in Table \@ref(tab:modelcomparison3). This posits the general question of the selection of fixed covariates. Estimating an AME rank 2 network model *without any* fixed effects did *not* depress substantially the representation of network dependencies in the estimated model. In practice, this makes covariate selection troublesome.

Lastly, longer burn-in and sampling durations could potentially improve the estimates. However, drastically raising these numbers would push computational feasibility. To verify robustness with respect to the length of the MCMC estimation, I raise the number of iterations to 400,000 after a burn-in period of 100,000.^[The models of the first comparison took about two hours to estimate. Quadrupling the number of iterations in the last model required about ten hours to compute.] Output density is kept the same. Posterior estimates for this model are in Table \@ref(tab:modelcomparison3). The intercept estimate is lower, and the estimate for *ConflictInd* is now negative, although still non-significant. The other fixed-effects estimates are qualitative as before. Moreover, the effective sample size has *not* improved. In the random effects, we see drastically increased variance of the additive effect and the corresponding standard error. This is also apparent upon further inspection of the trace plots in Figure \@ref(fig:ame4xtrace). Figure \@ref(fig:gofstats100vs400) shows a comparison of the posterior goodness of fit statistics for both AME rank 2 models, with the only difference in the length of the MCMC estimation. The increase yields narrower distributions for the goodness of fit statistics but does not improve on general model fit.

```{r modelsextended}
fitGIPKVP <- readRDS(file = "models/fitGIPKVP.rds") # AME Drop all non significant 
fitRU7J8K <- readRDS(file = "models/fitRU7J8K.rds") # AME quadruble iterations
```

```{r modelcomparison3, results='asis'}

modelAMErestr <- rbind(table_ame(fitGIPKVP)[1:3, ], rep(" ", 3), table_ame(fitGIPKVP)[4:5, ], 
                       rep(" ", 3), table_ame(fitGIPKVP)[6, ], rep(" ", 3), table_ame(fitGIPKVP)[7:8, ])
  
modelAMErestr <- cbind(modelAMErestr, table_ame(fitRU7J8K))
rownames(modelAMErestr) <- rownames(table_ame(fitRU7J8K))
rownames(modelAMErestr)[1] <- "Intercept"

modelAME1msg <- paste("Effective Sample Size: ", toString(round(coda::effectiveSize(fitGIPKVP$BETA))), ".", sep = "") 
modelAME2msg <- paste("Effective Sample Size: ", toString(round(coda::effectiveSize(fitRU7J8K$BETA))), ".", sep = "") 

kbl(modelAMErestr, booktabs = T, caption = "Model Comparison") %>%
  kable_styling(full_width = T) %>%
  column_spec(1, width = "4cm") %>%
  pack_rows("AME Components", 10, 11, latex_gap_space = "1em") %>%
  add_header_above(c(" " = 1 , "AME (R=2, selected)" = 3, "AME (R=2, 4x)" = 3)) %>%
  footnote(number = c(modelAME1msg, modelAME2msg))
```

```{r ame4xtrace, fig.cap ="Trace plots for estimated posterior variance of the additive effect and the intercept. Results based on 400,000 iterations and a burn-in period of 100,000. Chains were thinned by keeping every $100^{th}$ sample.", fig.align='center', out.width="95%"}
knitr::include_graphics("figures/ame4xtraceplot.pdf")
```

```{r gofstats100vs400, fig.cap ="Posterior Goodness of Fit statistics for model estimates with 100,000 iterations (grey) and 400,000 iterations (green). A narrower distribution can be identified for the green model.", fig.align='center', out.width="95%"}
knitr::include_graphics("figures/gofstats100vs400.pdf")
```


## Limitations and Further Approaches
Before entering the limitations and the discussion on the AME network model, two possible approaches to improve the present issues.

*1.* Changes in model specifications above did only affect dyadic covariates. Given the persistent trend in the variance of the additive effect and the intercept, one could hypothesize that the actor-specific propensity of tie formation is not sufficiently considered within the models' fixed effects. This could be alleviated by adding more nodal covariates. Unfortunately, there was no evident political or economic factor that could have been included. One option could be to construct an index of vote patterns in the United Nations, along the lines of the outcome variable in @dynamicAMEN. This is left for further exploration. 

*2.* Wide posterior intervals indicate a high uncertainty about the parameter estimates of the intercept and the variance of the additive effects. This could present problems with the posterior sampling.^[See for example @mcmcissues.] A natural step would be to specify different starting values as well as stronger priors, specifically for the intercept. The network density^[This calculation takes into account every state in the data, it therefore slightly understimates the density provided before.] ranges from `r round(range(gden(allyNet, mode = "digraph"))[1], 3)` to `r round(range(gden(allyNet, mode = "digraph"))[2], 3)` in the time-frame 1981 - 2000. Assuming this low density is a reliable characteristic of the interstate alliance network, a narrower prior distribution could be selected for the intercept.

The low effective sample size and the troublesome trace plots of the AME network model leave doubts about the validity of the statistical inference in this particular application case. Further analysis to improve the model fitting procedure would need to tackle the MCMC estimation procedure with more elaborated tools on the analysis of different chains as the visual approach provided in this seminar paper. The present issues could have their roots in different aspects of the present data set. The absence of strong predictors for the actor-specific propensity of tie formation, for example. A very low network density, along with large heterogeneity in the network actors too; as well as the symmetric, binary outcome.


# Discussion
The Additive and Multiplicative Effects network model provides an extension of the already familiar generalized linear model framework to network data. By adding a node-specific additive random effect and a latent factor as a multiplicative effect, an AME model can capture a range of statistical network effects that are inherent in network data and ordinary regression frameworks fail to account for. The deliberate omission of these dependencies in the data can lead to significantly biased estimates of effect sizes. Moreover, these network statistics could contain valuable information about actors in the network themselves and their respective relationships. In this regard, the AME Model presented in @Hoff2021 provides an easy-to-use and interpretable model for network data. 

The preceding comparison of model specifications indicated the importance of accounting for all types of present network effects. In addition, it exposed the issues of an ordinary regression approach in generating networks that do exhibit a similar amount of network effects, thus question the validity of statistical inference. The AME network model does account for these effects and exhibits a good statistical fit with respect to the network statistics. The estimated effects of the nodal and dyadic covariates in the AME network model are directionally consistent with the literature on the determinants of interstate defence alliances. Further analysis of the AME random effects did provide additional important features about the composition of the discussed network, as well as capturing some well-known truisms. 

Notwithstanding, the Bayesian estimation procedure makes it difficult to pin down issues in the estimation, for example in this paper, and thus confidently establish valid statistical inference. It requires furthermore a different approach to model assessment and goodness of fit, specifically with covariate selection in mind. With the added benefit of the multiplicative effect capturing omitted variables, the omission of potentially relevant predictors is probably less distorting than in a classical regression approach. The quality of results although may vary and strongly depend on network size, density, and available predictors, in combination with the type of outcome variable and available data. Yet, as a versatile approach in statistical modelling of networks, it can uncover interesting features about the data at hand, as for example in the preceding application. In conjunction with other modelling approaches, it thus can provide an important tool in the analysis of network data.

\newpage
# References
